{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ebe5a0e5-7ecd-4281-a7b6-cad692402730",
   "metadata": {},
   "source": [
    "# Lemmatization, pos tagging, and digitized books\n",
    "\n",
    "Lemmatization of words is to find the base form of a word. The base form is called a lemma. \n",
    "\n",
    "To do this, we need to know the grammatical category of words; noun, verb, adjective, or adverb. Because the same word can have different base form depending on its word class. The process of getting to know the grammatical category of words is called \"Part-Of-Speech tagging\".\n",
    "\n",
    "The NLTK library can be used in this process. Other libraries can be used as well, for example Spacy.\n",
    "\n",
    "In the script below, the `nltk.pos_tag` is used to add the grammatical category to each word. These tags must then be converted to WordNet format using a function called `pos_tagger`.\n",
    "\n",
    "WordNet is a comprehensive lexical database for the English language, where words are organized into synsets (synonym sets) based on their meaning.\n",
    "\n",
    "WordNet uses specific conventions to represent grammatical categories: `wordnet.ADJ` for adjectives, `wordnet.VERB` for verbs, `wordnet.NOUN` for nouns, and `wordnet.ADV` for adverbs.\n",
    "\n",
    "When POS-tags are converted to WordNet format, the `WordNetLemmatizer` can be used to lemmatize the words correctly. If a word does not have an available POS-tag, then the word is included without changes.\n",
    "\n",
    "\n",
    "A lemmatization of the text data will often be part of the preprocessing of the text data, and it will occur as a prelude to an analysis. This means that before we get to lemmatization, we need to load the text, clean it, and remove stop words. After lemmatization, we perform a visualization of some of the words that frequently appear in the text. The visualization is intended as an example of why lemmatization can be a smart thing to do."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585c4287-ec9a-4867-9b81-d909720af426",
   "metadata": {},
   "source": [
    "## Dowload a digitized book from the Royal Danish Library\n",
    "\n",
    "In this note book we wil work with the book _[Carr, John. A Northern Summer: Or Travels Round the Baltic, through Denmark, Sweden, Russia, Prussia, and Part of Germany, in the Year 1804. 1805.\n",
    "](https://soeg.kb.dk/permalink/45KBDK_KGL/1pioq0f/alma99122600818605763)_\n",
    "\n",
    "The deals with travel experiences, cultural observations, and historical contexts countries around the Baltic Sea,\n",
    "\n",
    "The book was digitized in 2017 and was originally categorized under the subject History - The Three Nordic Kingdoms.\n",
    " \n",
    "This is the link to the online edition: [https://www.kb.dk/e-mat/dod/130022078187_bw.pdf](https://www.kb.dk/e-mat/dod/130022078187_bw.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c57c71-1c1d-4631-abec-d15fa93360ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install PyPDF2\n",
    "import requests\n",
    "from io import BytesIO\n",
    "from PyPDF2 import PdfReader\n",
    "\n",
    "# URL to the ocr scanned, pdf verison of the text\n",
    "url = \"https://www.kb.dk/e-mat/dod/130022078187_bw.pdf\"\n",
    "\n",
    "# Download the pdf file\n",
    "response = requests.get(url)\n",
    "response.raise_for_status()  # Check if the request was successful\n",
    "\n",
    "# Open the pdf file in memory\n",
    "pdf_file = BytesIO(response.content)\n",
    "\n",
    "# Create a PDF reader object\n",
    "reader = PdfReader(pdf_file)\n",
    "\n",
    "# Extract text from each page starting from page 5\n",
    "text_content = []\n",
    "for page in reader.pages[7:]:\n",
    "    text_content.append(page.extract_text())\n",
    "\n",
    "# Join all the text content into a single string\n",
    "full_text = \"\\n\".join(text_content)\n",
    "\n",
    "# Print the extracted text\n",
    "print(full_text[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d923f66e-c986-43b7-8af4-b412703956fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def clean(text): \n",
    "\n",
    "    # solving a hyphen-newline issue. \n",
    "    text = text.replace('-\\n', '')\n",
    "    \n",
    "    # match a variety of punctuation and special characters\n",
    "    # backslash \\ and the pipe symbols | plays important roles, for example here \\? \n",
    "    # Now it is a good idea to look up a see what \\ and | does \n",
    "    text = re.sub(r'\\.|«|,|:|;|!|\\?|\\(|\\)|\\||\\+|\\'|\\\"|‘|’|“|”|\\'|\\’|…|\\-|_|–|—|\\$|&|\\*|>|<|\\/|\\[|\\]', ' ', text)\n",
    "\n",
    "    # Regex pattern to match numbers and words containing numbers\n",
    "    text = re.sub(r'\\b\\w*\\d\\w*\\b', '', text)\n",
    "    \n",
    "    # remove all characters that are not letters\n",
    "    text = re.sub(r'[^a-zA-Z]+', ' ', text)\n",
    "  \n",
    "    # Remove words with length 2 or less\n",
    "    text = re.sub(r'\\b\\w{1,2}\\b', '', text)\n",
    "    \n",
    "    # sequences of white spaces \n",
    "    text = re.sub(r'\\s+', ' ', text) \n",
    "\n",
    "    # lower the letters\n",
    "    text = text.lower()\n",
    "\n",
    "    # return the text\n",
    "    return text\n",
    "    \n",
    "\n",
    "clean_full_text = clean(full_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b563775-ae7d-444b-845c-7c0fcc5c5d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_full_text[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd6ed7f3-e4b6-439c-8bcc-4b2c0cd5ec17",
   "metadata": {},
   "source": [
    "## Remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0903e98-0b9b-48b7-a1d8-6c6cdd02f0bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove stopwords\n",
    "import urllib.request\n",
    "\n",
    "#import an English stopword list\n",
    "url = \"https://sciencedata.dk/shared/5dad7af4b0c257e968111dd0ce19eb99?download\"\n",
    "en_stop_words = urllib.request.urlopen(url).read().decode().split()\n",
    "# Add additional stopwords using Pythons list append() method\n",
    "en_stop_words.extend(['■', '%', '»', '•', '^', '\\\\', '-'])\n",
    "\n",
    "# text data in\n",
    "text = clean_full_text\n",
    "\n",
    "# Change text to wordlist\n",
    "tokens = text.split()\n",
    "tokens_wo_stopwords = [i for i in tokens if i.lower() not in en_stop_words]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "836df69d-767b-4fae-80cc-dd2e182a05fb",
   "metadata": {},
   "source": [
    "## Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40bcc7e9-1382-436a-b07c-18912bf49072",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the Natural Language Toolkit (nltk) library\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Downloading the 'averaged_perceptron_tagger' model, which is used for part-of-speech tagging\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# Importing the WordNet corpus from nltk, which is used for lemmatization\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "# Initializing the WordNetLemmatizer, which is used to lemmatize words\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Function to convert nltk POS tags to WordNet POS tags\n",
    "def pos_tagger(nltk_tag):\n",
    "    if nltk_tag.startswith('J'):\n",
    "        return wordnet.ADJ  # Adjective\n",
    "    elif nltk_tag.startswith('V'):\n",
    "        return wordnet.VERB  # Verb\n",
    "    elif nltk_tag.startswith('N'):\n",
    "        return wordnet.NOUN  # Noun\n",
    "    elif nltk_tag.startswith('R'):\n",
    "        return wordnet.ADV  # Adverb\n",
    "    else:\n",
    "        return None  # If the tag doesn't match, return None\n",
    "\n",
    "# Send in a list of words\n",
    "in_data_list = tokens_wo_stopwords\n",
    "\n",
    "# Performing part-of-speech tagging on the input data list\n",
    "pos_tagged = nltk.pos_tag(in_data_list)\n",
    "\n",
    "# Mapping the nltk POS tags to WordNet POS tags\n",
    "wordnet_tagged = list(map(lambda x: (x[0], pos_tagger(x[1])), pos_tagged))\n",
    "\n",
    "# Initializing an empty list to store lemmatized words\n",
    "lemmatized_word_list = []\n",
    "\n",
    "# Iterating over the word and its corresponding WordNet POS tag\n",
    "for word, tag in wordnet_tagged:\n",
    "    if tag is None:\n",
    "        # If no valid WordNet POS tag, append the original word\n",
    "        lemmatized_word_list.append(word)\n",
    "    else:\n",
    "        # Lemmatize the word using the WordNet POS tag\n",
    "        lemmatized_word_list.append(lemmatizer.lemmatize(word, tag))\n",
    "\n",
    "# List of lemmatized words\n",
    "#lemmatized_word_list[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609c451b-99e8-42cd-93e9-af4b74789f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fdist_filtered = nltk.FreqDist(lemmatized_word_list).plot(20, title='Some of the most frequent words')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e47ca5d-d5b4-498e-a947-ec4bad0d5be8",
   "metadata": {},
   "source": [
    "### Sources:\n",
    "- https://www.nltk.org/book/ch05.html\n",
    "- https://www.nltk.org/howto/wordnet.html\n",
    "- https://www.geeksforgeeks.org/python-lemmatization-approaches-with-examples/ \n",
    "- https://medium.com/@kevinnjagi83/lemmatization-in-nlp-2a61012c5d66\n",
    "- [Carr, John. A Northern Summer: Or Travels Round the Baltic, through Denmark, Sweden, Russia, Prussia, and Part of Germany, in the Year 1804. 1805.\n",
    "](https://soeg.kb.dk/permalink/45KBDK_KGL/1pioq0f/alma99122600818605763)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
