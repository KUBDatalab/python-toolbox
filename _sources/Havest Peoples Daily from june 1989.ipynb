{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f617537-9929-4bcc-8afa-87a9dd7231eb",
   "metadata": {},
   "source": [
    "# Havest Peoples Daily from june 1989\n",
    "\n",
    "The purpose is to gain knowledge about how to scrape text from the web using Python and the BeautifulSoup library."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8051f1d-f10f-4c71-87c7-ccb7cabfda32",
   "metadata": {},
   "source": [
    "````Python\n",
    "# Import libraries\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f208f7d-82cc-44ea-9e12-963f8080e49a",
   "metadata": {},
   "source": [
    "On 'https://www.laoziliao.net/rmrb/' you can find old editions of [People's Daily](https://en.wikipedia.org/wiki/People%27s_Daily).\n",
    "\n",
    "In this notebook I show how to harwest text from June 1989\n",
    "\n",
    "Start by retrieving links located on the page 'https://www.laoziliao.net/rmrb/1989-06'\n",
    "\n",
    "Links are located in a div box called month box."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2048c81b-5903-4374-9c33-09535ab25725",
   "metadata": {},
   "source": [
    "````Python\n",
    "# url to papers from June 1989\n",
    "url = 'https://www.laoziliao.net/rmrb/1989-06'\n",
    "\n",
    "# download html\n",
    "headers={'name': 'Add your name here.', 'e-mail': 'add e-mail'}\n",
    "\n",
    "response = requests.get(url, headers=headers)\n",
    "\n",
    "soup = bs(response.text, 'html.parser')\n",
    "\n",
    "# get the html in the div tag with the attribute 'id' with the value month_box\n",
    "month_box = soup.find('div', {'id' : 'month_box'})\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bafce68c-fbdf-4034-978b-0ed017e902e7",
   "metadata": {},
   "source": [
    "When we dive into the html we can see that we need to find all the 'a' tags in the month box to get links to go a layer deeper into the page. \n",
    "\n",
    "When we do that, we can find the links that take us to pages with articles related to each day of the month.\n",
    "\n",
    "We can see that the dates are stated in the URLs.\n",
    "\n",
    "See for example \"1989-06-01\" in the first link.\n",
    "\n",
    "To extract the links we most loop the list and apply .get('href') to each item."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4960d801-1fe4-4dd5-b347-d69de990969e",
   "metadata": {},
   "source": [
    "````Python\n",
    "a_tags = month_box.find_all('a')# loop through the list of 'a' tags and get every hyper reference for each day\n",
    "\n",
    "links = [i.get('href') for i in a_tags]\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d95226a-2254-40ac-88e1-e499b6448b88",
   "metadata": {},
   "source": [
    "Each link goes to a subpage, and the HTML of the pages must be downloaded and inspected."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "433526f9-1431-4bca-870d-7f258a26635c",
   "metadata": {},
   "source": [
    "````Python\n",
    "def get_soup(url):\n",
    "    # download html\n",
    "    headers={'name': 'add name', 'e-mail': 'add email'}\n",
    "    \n",
    "    response = requests.get(url, headers=headers)\n",
    "    \n",
    "    soup = bs(response.text, 'html.parser')\n",
    "\n",
    "    time.sleep(random.randint(0, 3))\n",
    "\n",
    "    return soup\n",
    "\n",
    "soups_from_each_day = [get_soup(html) for html in links]\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "710b9ce5-6a86-4fce-9e97-ae82f8b0bd9d",
   "metadata": {},
   "source": [
    "A close reading of the HTML shows that the pages are set up so that links to different articles are located in the div box '\\<div class=\"card mt-2\">'.\n",
    "\n",
    "After this, we end up with a list of lists, which we pass back to one list.\n",
    "\n",
    "The different links contain both links to articles and the article titles. We store the links in one long list and the titles in another. We store both lists in a dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef03c6d-fe78-4d75-bda2-adb778d04905",
   "metadata": {},
   "source": [
    "````Python\n",
    "lists_of_a_tags_linking_to_individual_articles = [] \n",
    "\n",
    "def get_links_to_articles(another_soups):\n",
    "    card_mt2 = another_soups.find_all('div', {'class' : 'card mt-2'})\n",
    "    \n",
    "    def get_a_tags(i):\n",
    "        return i.find_all('a')\n",
    "    \n",
    "    listoflists = [get_a_tags(i) for i in card_mt2]\n",
    "    \n",
    "    a_tag_list = [i for y in listoflists for i in y]\n",
    "    \n",
    "    return a_tag_list\n",
    "\n",
    "\n",
    "lists_of_a_tags_linking_to_individual_articles = [get_links_to_articles(another_soup) for another_soup in soups_from_each_day]\n",
    "flatten_lists_of_a_tags_linking_to_individual_articles = [i for y in lists_of_a_tags_linking_to_individual_articles for i in y]\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a95de72-31a9-49f8-a8f0-939448098556",
   "metadata": {},
   "source": [
    "````Python\n",
    "links_to_individual_articles = [i.get('href') for i in flatten_lists_of_a_tags_linking_to_individual_articles\n",
    "]\n",
    "\n",
    "titles_of_individual_articles = [i.text for i in flatten_lists_of_a_tags_linking_to_individual_articles\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "df = pd.DataFrame({'links': links_to_individual_articles,\n",
    "                  'titles': titles_of_individual_articles})\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "403a8044-e42c-4565-aa76-82abd07a5494",
   "metadata": {},
   "source": [
    "1760 articles from June 1989 have been found. Each article must be downloaded. A relatively complicated analysis is needed to understand how to best make this download process happen.\n",
    "\n",
    "It then takes quite a long time to download the articles, because 1760 requests must be made.\n",
    "\n",
    "When the articles have been retrieved, they are sent to our dataframe and stored there."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ad6943-97ec-4f2f-b085-3a09d5b145ae",
   "metadata": {},
   "source": [
    "````Python\n",
    "raw_texts = []\n",
    "for i in links_to_individual_articles:\n",
    "    # identify the article id in the link\n",
    "    get_article_id = i.split('#')[-1]\n",
    "    just_another_soup = get_soup(i)\n",
    "    card_mt2 = just_another_soup.find_all('div', {'class' : 'card mt-2'})\n",
    "    for j in card_mt2:\n",
    "        # identify the card_mt2 element that holds the article id\n",
    "        if get_article_id in str(j):\n",
    "            raw_texts.append(j.text)\n",
    "\n",
    "# add the texts to the dataframe\n",
    "df['text'] = raw_texts\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6975ff2-09bf-4b41-9f36-67303386e78c",
   "metadata": {},
   "source": [
    "We save this Dataframe as both .pkl and as .tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "314a1890-b9bb-4263-a81f-ec9fb8238a01",
   "metadata": {},
   "source": [
    "````Python\n",
    "df.to_pickle(\"rmrb_june_1989.pkl\")\n",
    "\n",
    "df.to_csv('rmrb_june_1989.tsv', sep='\\t', index=False, header=True)\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce964213-5186-4eb0-a985-6fefdc5ddddd",
   "metadata": {},
   "source": [
    "Later we can import the data again like this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a0de6e-7d9a-4b50-8278-0e00bcb44731",
   "metadata": {},
   "source": [
    "````Python\n",
    "import pandas as pd\n",
    "df1 = pd.read_table('rmrb_june_1989.tsv',delimiter='\\t')\n",
    "````"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0169fdbe-16af-4aa0-9440-4670bc5e2cde",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
