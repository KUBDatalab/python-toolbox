{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1b59e99-a254-4bae-ad7c-29dcf8cb8183",
   "metadata": {},
   "source": [
    "# Collocation and digitized books\n",
    "\n",
    "In this notebook we tune in on the concept of collocation. Collocation is defined as the combination of two or more words that usually appear frequently together, e.g., bear responsibility or weighty arguments. _Source: [\"kollokation\" in ordnet.dk](https://ordnet.dk/ddo/ordbog?query=kollokation)_\n",
    "\n",
    "Different approaches is being used to find words that collocates is texts. In the NLTK library the collocation tool is build around n-grams and measuring of association. _Source: [NLTK, Documentation, Collocations](https://www.nltk.org/howto/collocations.html)_ The collocation script in this notebook is different. It works by define a \"window size\" around a keyword. The \"window size\" is for example 10 words before and after the keyword. The script then count words in the window, excluding the keyword itself. Into the script goes a list of words, and I suggest that you send in a word list without stopwords, as this would often give the most useful results.\n",
    "\n",
    "The results can be usefull to get an understanding of the context and semantic around the selected keywords.\n",
    "\n",
    "\n",
    "We will use collocations to look into this more than 00 years old book for travellers in the Nordic periphery of Europe.\n",
    "\n",
    "  [Macdonald, James. Travels through Denmark and Part of Sweden during the Winter and Spring of the Year 1809 : Containing Authentic Particulars of the Domestic Condition of Those Countries, the Opinions of the Inhabitants, and the State of Agriculture. 2015.](https://soeg.kb.dk/permalink/45KBDK_KGL/1pioq0f/alma99122806627205763)\n",
    "\n",
    "The book has been digitized by The Royal Danish Library in 2015, and it is available on the url: [https://www.kb.dk/e-mat/dod/130014244515_bw.pdf](https://www.kb.dk/e-mat/dod/130014244515_bw.pdf)\n",
    "\n",
    "\n",
    "## Download the book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5fba6d2-16bc-4af3-b605-21b50ce2b9bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 .\n",
      "I.’ V \n"
     ]
    }
   ],
   "source": [
    "#! pip install PyPDF2\n",
    "import requests\n",
    "from io import BytesIO\n",
    "from PyPDF2 import PdfReader\n",
    "\n",
    "# URL to the ocr scanned, pdf verison of the text\n",
    "url = \"https://www.kb.dk/e-mat/dod/130014244515_bw.pdf\"\n",
    "\n",
    "# Download the pdf file\n",
    "response = requests.get(url)\n",
    "response.raise_for_status()  # Check if the request was successful\n",
    "\n",
    "# Open the pdf file in memory\n",
    "pdf_file = BytesIO(response.content)\n",
    "\n",
    "# Create a PDF reader object\n",
    "reader = PdfReader(pdf_file)\n",
    "\n",
    "# Extract text from each page starting from page 5\n",
    "text_content = []\n",
    "for page in reader.pages[6:]:\n",
    "    text_content.append(page.extract_text())\n",
    "\n",
    "# Join all the text content into a single string\n",
    "full_text = \"\\n\".join(text_content)\n",
    "\n",
    "# Print the extracted text\n",
    "print(full_text[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69450fe5-ddbe-4596-9874-4b8b8adf738e",
   "metadata": {},
   "source": [
    "## Preprocess the text\n",
    "\n",
    "Next step is to send the text through a scrubbing pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a43cf0d7-d91c-41f5-929f-3062fcab52d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def clean(text): \n",
    "\n",
    "    # solving a hyphen-newline issue. \n",
    "    text = text.replace('-\\n', '')\n",
    "    \n",
    "    # match a variety of punctuation and special characters\n",
    "    # backslash \\ and the pipe symbols | plays important roles, for example here \\? \n",
    "    # Now it is a good idea to look up a see what \\ and | does \n",
    "    text = re.sub(r'\\.|«|,|:|;|!|\\?|\\(|\\)|\\||\\+|\\'|\\\"|‘|’|“|”|\\'|\\’|…|\\-|_|–|—|\\$|&|\\*|>|<|\\/|\\[|\\]', ' ', text)\n",
    "\n",
    "    # Regex pattern to match numbers and words containing numbers\n",
    "    text = re.sub(r'\\b\\w*\\d\\w*\\b', '', text)\n",
    "  \n",
    "    # Remove words with length 2 or less\n",
    "    text = re.sub(r'\\b\\w{1,2}\\b', '', text)\n",
    "    \n",
    "    # sequences of white spaces \n",
    "    text = re.sub(r'\\s+', ' ', text) \n",
    "\n",
    "    # lower the letters\n",
    "    text = text.lower()\n",
    "\n",
    "    # return the text\n",
    "    return text\n",
    "    \n",
    "\n",
    "clean_full_text = clean(full_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85bc6e73-c544-476a-bf4b-5ad9182bb055",
   "metadata": {},
   "source": [
    "## Remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2269c52d-b277-419f-bf3b-f7d1a712f3e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove stopwords\n",
    "import urllib.request\n",
    "\n",
    "#import an English stopword list\n",
    "url = \"https://sciencedata.dk/shared/5dad7af4b0c257e968111dd0ce19eb99?download\"\n",
    "en_stop_words = urllib.request.urlopen(url).read().decode().split()\n",
    "# Add additional stopwords using Pythons list append() method\n",
    "en_stop_words.extend(['■', '%'])\n",
    "\n",
    "# text data in\n",
    "text = clean_full_text\n",
    "\n",
    "# Change text to wordlist\n",
    "tokens = text.split()\n",
    "tokens_wo_stopwords = [i for i in tokens if i.lower() not in en_stop_words]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00816645-359b-4b5b-b8c2-b7ab15fa14ec",
   "metadata": {},
   "source": [
    "## Collocation\n",
    "\n",
    "Add or replace the keywords with your own words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0223f0c7-aa28-4825-9e01-711d7e692a5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'belt': Counter({'great': 6, 'miles': 3, 'description': 3, 'danes': 3}),\n",
       " 'sound': Counter({'place': 4,\n",
       "          'ice': 4,\n",
       "          'marble': 4,\n",
       "          'helsingborg': 3,\n",
       "          'great': 3})}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "in_data_list = tokens_wo_stopwords\n",
    "\n",
    "keywords = ['belt', 'sound']\n",
    "\n",
    "\n",
    "keyword_proximity_counts = {keyword: Counter() for keyword in keywords}\n",
    "window_size = 10\n",
    "\n",
    "for i, token in enumerate(in_data_list):\n",
    "    if token in keywords:\n",
    "        # Define the window around the keyword\n",
    "        start = max(0, i - window_size)\n",
    "        end = min(len(in_data_list), i + window_size + 1)\n",
    "        # Count terms in the window, excluding the keyword itself\n",
    "        for j in range(start, end):\n",
    "            if j != i:\n",
    "                keyword_proximity_counts[token][in_data_list[j]] += 1\n",
    "\n",
    "# Filter out terms with counts less than x (count >= x)\n",
    "filtered_keyword_proximity_counts = {\n",
    "    keyword: Counter({term: count for term, count in counts.items() if count >= 3})\n",
    "    for keyword, counts in keyword_proximity_counts.items()\n",
    "}\n",
    "\n",
    "filtered_keyword_proximity_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a95350-a27e-469a-b7fc-2ca5f82408e6",
   "metadata": {},
   "source": [
    "## Explanation of the collocation algorithm\n",
    "\n",
    "1. **Input Data**:\n",
    "   - `in_data_list`: This is a list of tokens (words) from which stopwords have been removed. \n",
    "   - `keywords`: A list of specific keywords (`['fiord', 'belt', 'sound']`) that you are interested in analyzing within the `in_data_list`.\n",
    "\n",
    "2. **Data Structures**:\n",
    "   - `keyword_proximity_counts`: A dictionary where each keyword is associated with a `Counter` object. This `Counter` will keep track of how often other terms appear near the keyword within a specified window.\n",
    "\n",
    "3. **Parameters**:\n",
    "   - `window_size`: This is set to 10, meaning that the script will consider a window of 10 tokens before and after each occurrence of a keyword in the `in_data_list`.\n",
    "\n",
    "4. **Main Loop**:\n",
    "   - The script iterates over each token in `in_data_list` using `enumerate` to get both the index (`i`) and the token itself.\n",
    "   - If the token is one of the specified `keywords`, the script defines a \"window\" around this keyword:\n",
    "     - `start`: The beginning of the window, calculated as `max(0, i - window_size)`. This ensures the window doesn't start before the list begins.\n",
    "     - `end`: The end of the window, calculated as `min(len(in_data_list), i + window_size + 1)`. This ensures the window doesn't extend beyond the list.\n",
    "   - Within this window, the script counts the occurrence of each term, excluding the keyword itself. This is done using another loop over the indices from `start` to `end`. If the current index `j` is not equal to `i` (the index of the keyword), the term at `in_data_list[j]` is counted in the `Counter` for that keyword.\n",
    "\n",
    "5. **Filtering**:\n",
    "   - After populating `keyword_proximity_counts`, the script filters out terms that appear less than a specified number of times (in this case, 4 times) near each keyword.\n",
    "   - This is done using a dictionary comprehension that creates a new dictionary, `filtered_keyword_proximity_counts`. For each keyword, it creates a new `Counter` that only includes terms with a count of 4 or more.\n",
    "\n",
    "6. **Output**:\n",
    "   - `filtered_keyword_proximity_counts`: This is the final result, a dictionary where each keyword is associated with a `Counter` of terms that frequently appear near it, filtered to only include those with a count of at least 4.\n",
    "  \n",
    "_NB: Created using chatGPT-4-omni._ "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
