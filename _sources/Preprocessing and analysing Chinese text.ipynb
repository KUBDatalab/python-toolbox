{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing and analysing Chinese text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The course covers basic Python code that can get you started using programming as a tool for text processing, quantitative analysis and text and data mining.\n",
    "\n",
    "In more technical terms, we review concepts such as variables, values, the data types text strings, lists and loops.\n",
    "\n",
    "We go through an example of how to retrieve text data, prepare data and use the jieba library. Jieba is used to divide text into words and subdivides traditional Chinese.\n",
    "\n",
    "_Source: https://github.com/fxsjy/jieba_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Webscrape libraries\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "# For preprocessing and analysing\n",
    "import jieba\n",
    "import nltk\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a variable that we use to store the url to the pages that we want to webscrape.\n",
    "\n",
    "We need to scrape this wikipedia page: 反对逃犯条例修订草案运动"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the url in a variable\n",
    "url_zh = 'https://zh.wikipedia.org/zh-cn/%E5%8F%8D%E5%B0%8D%E9%80%83%E7%8A%AF%E6%A2%9D%E4%BE%8B%E4%BF%AE%E8%A8%82%E8%8D%89%E6%A1%88%E9%81%8B%E5%8B%95'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We insert one of the variable names into request.get('url') below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get data\n",
    "page = requests.get(url_zh)\n",
    "\n",
    "# scrape webpage\n",
    "soup = BeautifulSoup(page.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find all 'headline3 and paragraph-tags'\n",
    "tags = soup.find_all(['h1', 'h3', 'p'])\n",
    "\n",
    "# Parse the text from the p_tags ajd 'join' a returned list into the variable called 'text'\n",
    "text = ' '.join([p.get_text() for p in tags]).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text [0:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation of text\n",
    "### Cleaning\n",
    "\n",
    "The text consists of Latin letters and Chinese characters.\n",
    "\n",
    "If you want to sort out the Latin letters, you can use the code below.\n",
    "\n",
    "Sources:\n",
    "\n",
    "https://stackoverflow.com/questions/2718196/find-all-chinese-text-in-a-string-using-python-and-regex\n",
    "\n",
    "https://unicode-table.com/en/blocks/cjk-unified-ideographs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chinese_list = re.findall(r'[\\u4e00-\\u9fff]+', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chinese_list[0:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The list can then be assembled into a text string again with .join()\n",
    "\n",
    "_Source: \"https://www.w3schools.com/python/ref_string_join.asp\"_   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chinese_text = ' '.join(chinese_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chinese_text[0:200]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text segmentation / tokenisation\n",
    "\n",
    "In the jieba.lcut method, we insert a text string or, as in this case, a variable containing a text string, and we control the cut mode. The L in .lcut() indicates that the method returns a list. 'Cut_all=True' should give the most possible hyphenation of the text, be fast, but less accurate. 'Cut_all=False' should be more accurate than the first, and thus more suitable for text analysis. _Source: https://github.com/fxsjy/jieba_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_list1 = jieba.lcut(chinese_text, cut_all=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_list1[0:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get returned many fields consisting of 'white_space'.\n",
    "\n",
    "To see these lines removed from our data, we use 'if' to put a condition into the code. We write, if our lines consist of characters that are not equal to 'white space', then we are interested in storing it in the variable seg_list2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_list2 = [item for item in seg_list1 if item != ' ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_list2[0:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown above, lists are made using square brackets ( [ ] ).\n",
    "\n",
    "You can access the elements in the list by referring to the index number. Again, we can use both positive and negative numbers. Remember that in Python the first index number is 0 and not 1, which means we access the first and last element of the list like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (seg_list2[0])\n",
    "print (seg_list2[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part of Speech Tagging (POS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jieba's part of speech tagger returns the words and tags in two different elements. To use the pos tagger, import 'import jieba.posseg as pseg'.\n",
    "\n",
    "According to the documentation, you use pseg after .cut( 'text_string' ). Source: \"4. Part of Speech Tagging https://github.com/fxsjy/jieba\"\n",
    "\n",
    "We get returned words and tags. They are in .word and .flag. In the documentation, the programmer shows how to print words and tags, but I would like to have all words and tags stored as pairs in a list. Therefore I use a tuple which is a python data type and add each word and tag pair to a list which I call 'pos'. Source: Python Tuples https://www.w3schools.com/python/python_tuples.asp_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba.posseg as pseg\n",
    "words = pseg.cut(chinese_text)\n",
    "pos_tags = []\n",
    "for w in words:\n",
    "    if w.word > ' ':\n",
    "        word_tag = tuple((w.word, w.flag))\n",
    "        pos_tags.append(word_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_tags[0:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am writing a for loop that contains a condition ('if'). With the loop I go through the list of tuples. If the first element of the 'tuple' ([1]) is equal to 'v' I add the first element ([0]) to the list 'words'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = []\n",
    "for item in pos_tags:\n",
    "    if item[1] == 'v':\n",
    "        words.append(item[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words[0:20] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before looking at the distribution of words with 'v' tags, we need to address the fact that python by default cannot print Chinese characters. Therefore, we import 'matplotlib.pyplot as plt' and change font.family to \"Microsoft YaHei\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams[\"font.family\"] = \"Microsoft YaHei\"  # husk på mac skal man bruge  'Heiti TC'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After this we can import nltk and use nltk.FreqDist()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.FreqDist(pos_tags).plot(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task: try replacing 'v' with other tags.\n",
    "## Stop words\n",
    "\n",
    "Stop words are small words that are often not meaningful words.\n",
    "\n",
    "We therefore need to load a stop word list. It is online at Science data at this link: https://sciencedata.dk/shared/93a217a0533d949d9b2c675cd3c99cfd?download.\n",
    "\n",
    "To retrieve the file we use \"from urllib.request import urlopen\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "\n",
    "target_url = 'https://sciencedata.dk/shared/93a217a0533d949d9b2c675cd3c99cfd?download'\n",
    "\n",
    "sw_ch = urlopen(target_url).read().decode('utf-8').split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now all the texts can be filtered for stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_tokens = []\n",
    "for word in seg_list2:\n",
    "    if word not in sw_ch:\n",
    "        filtered_tokens.append(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now all the texts can be filtered for stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdist_filtered = nltk.FreqDist(filtered_tokens).plot(20, title='Hyppigste ord (uden stopord)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "long_tokens = []\n",
    "\n",
    "for word in filtered_tokens:\n",
    "    if len(word) > 4:\n",
    "        long_tokens.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdist_filtered = nltk.FreqDist(long_tokens).plot(20, title='Længste ord')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLTK methods\n",
    "\n",
    "I have used nltk many times, but never with Chinese text. We experiment and create an nltk text object which should allow us to use various nltk methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk_text = nltk.Text(seg_list2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "collocation_list() returns a list of the most common word pairs in the text. Note that in some versions of Python, collocation_list() does not work. If this is the case, try _collocations()_ instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk_text.collocation_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The concordance() method returns the context of a specific expression. The length of the output can be changed with the parameters in width and lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk_text.concordance('反对', lines=30, width=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To identify words that appear in a similar context, we can use the similar() method.\n",
    "\n",
    "I have a notion that the method gives better results the longer the text is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# similar til \"politi\"\n",
    "nltk_text.similar('警察')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use the generate() method to generate more or less coherent text based on an existing text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_gen = nltk_text.generate(150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
