{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6cfe14ec-27f8-4402-881b-652eb1933b76",
   "metadata": {},
   "source": [
    "## Harvest text from the web\n",
    "\n",
    "The purpose is to gain knowledge about how to scrape data from the web in order to obtain text data for digital text analysis.\n",
    "\n",
    "To achieve this, we start with a very brief introduction to HTML and continues with the libraries BeautifulSoup and Requests.\n",
    "\n",
    "\n",
    "## Introduction HTML\n",
    "\n",
    "Read your first html file, and let us inspect it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ae1cc7d-cddf-40f8-8872-aa99a02f15f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<div>\n",
      "\t<p>This is a paragraph</p>\n",
      "</div>\n"
     ]
    }
   ],
   "source": [
    "# Read your html file\n",
    "html_file_path = 'html1.html'  \n",
    "with open(html_file_path, 'r', encoding='utf-8') as file:\n",
    "    html_content = file.read()\n",
    "\n",
    "#print \n",
    "print(html_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dec5067-f6a8-49f6-a1eb-4b75728bd12e",
   "metadata": {},
   "source": [
    "The first line is **\\<div>.** \n",
    "It is a **html tag**. \n",
    "\n",
    "**<** is the opening of the tag. The word **div** is the type of tag, and it defines a section in a html document. \n",
    "The second line begins with a **\\<p>** tag, and it defines a paragraph. The tag is **nested**. Which is a way of saying that it is embedded inside the <div> section. The text _This is a paragraph_ is the text that will be displayed on a webpage. Now follows the **\\</p>**. The **</** indicates this is a closing tag. A closing tag means that the nested piece is over. This is opposite to the opening of the tag **<**.\n",
    "\n",
    "## Attributes\n",
    "\n",
    "Read another html file and look at the first line. Inside the div tag we have added an **attribute** called **class**. The attributes has \"content\" as a value. Besides class is **id** a common attribute.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b1c557f4-7126-4f40-a2d9-5959a0cfd6da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<div class=\"content\">\n",
      "\t<p>This is a paragraph</p>\n",
      "</div>\n"
     ]
    }
   ],
   "source": [
    "# Read your html file\n",
    "html_file_path = 'html2.html'  \n",
    "with open(html_file_path, 'r', encoding='utf-8') as file:\n",
    "    html_content = file.read()\n",
    "\n",
    "#print \n",
    "print(html_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a38e6db1-3a2b-4904-83aa-137a68d8ad94",
   "metadata": {},
   "source": [
    "## Working with HTML with Python\n",
    "\n",
    "You would often see, that when people are working with HTML with Python then they will use the libraries BeautifulSoup, Selenium, and Regex; or a combination of the three libraries.\n",
    "\n",
    "This notebook sticks with BeautifulSoup alone. \n",
    "\n",
    "To install BeautifulSoup you can use either [pip install beautifulsoup4](https://pypi.org/project/beautifulsoup4/) or [conda install anaconda::beautifulsoup4](https://anaconda.org/anaconda/beautifulsoup4) .\n",
    "\n",
    "When the library is installed, then import the library by writing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37bf01d5-ce22-4b9f-ae64-2beadd8a05d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as bs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a91a12-8d01-4bbb-81b7-f818563575a2",
   "metadata": {},
   "source": [
    "Now you can read a html file and work with the HTML in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e3fd17dd-b89a-421d-ab71-32b17586b58e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<html>\n",
      "\t<body>\t\n",
      "\t\t<div class=\"content\">\n",
      "            <h1>Title</h1>\n",
      "\t\t\t<p>My first paragraph</p>\n",
      "\t\t</div>\n",
      "\t\t\n",
      "\t\t<div class=\"other\">\n",
      "            <h2>My first headline</h2>\n",
      "\t\t\t<p>My second paragraph</p>\n",
      "\t\t</div>\n",
      "\t\t\n",
      "\t\t<div class=\"content\">\n",
      "            <h2>My second headline</h2>\n",
      "\t\t\t<p>My third paragraph</p>\n",
      "\t\t</div>\n",
      "\t</body>\n",
      "</html>\n",
      "\n",
      "\t\n"
     ]
    }
   ],
   "source": [
    "# Read your html file\n",
    "html_file_path = 'html3.html'  \n",
    "with open(html_file_path, 'r', encoding='utf-8') as file:\n",
    "    html_content = file.read()\n",
    "\n",
    "#print \n",
    "print(html_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26feaf85-2554-47ee-bc39-2157bc62e349",
   "metadata": {},
   "source": [
    "When working with beautifulSoup the convention is to store the html in a variable called soup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dca322b4-b2b7-4436-acdd-e8b8a96f96e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<html>\n",
      " <body>\n",
      "  <div class=\"content\">\n",
      "   <h1>\n",
      "    Title\n",
      "   </h1>\n",
      "   <p>\n",
      "    My first paragraph\n",
      "   </p>\n",
      "  </div>\n",
      "  <div class=\"other\">\n",
      "   <h2>\n",
      "    My first headline\n",
      "   </h2>\n",
      "   <p>\n",
      "    My second paragraph\n",
      "   </p>\n",
      "  </div>\n",
      "  <div class=\"content\">\n",
      "   <h2>\n",
      "    My second headline\n",
      "   </h2>\n",
      "   <p>\n",
      "    My third paragraph\n",
      "   </p>\n",
      "  </div>\n",
      " </body>\n",
      "</html>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "soup = bs(html_content, 'html.parser')\n",
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd3176f-f277-4aae-874d-737c7b5726db",
   "metadata": {},
   "source": [
    "### Find and find_all\n",
    "\n",
    ".find returns the first tag.\n",
    "\n",
    ".find_all returns a list of tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b20eb89-4e92-4ea2-939d-e49717f6713a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<div class=\"content\">\n",
       "<h1>Title</h1>\n",
       "<p>My first paragraph</p>\n",
       "</div>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find('div')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6da61f23-1a28-4ed3-aa11-6acc230d1034",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<div class=\"content\">\n",
       " <h1>Title</h1>\n",
       " <p>My first paragraph</p>\n",
       " </div>,\n",
       " <div class=\"other\">\n",
       " <h2>My first headline</h2>\n",
       " <p>My second paragraph</p>\n",
       " </div>,\n",
       " <div class=\"content\">\n",
       " <h2>My second headline</h2>\n",
       " <p>My third paragraph</p>\n",
       " </div>]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find_all('div')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb2d1ee3-a5b7-4b73-b05e-e369b70f023b",
   "metadata": {},
   "source": [
    "When you have a list, they can access each element in the list using the index number. [0] is the first element."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9873a9e4-9e59-484e-9b41-fb8ed4e80d50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<div class=\"content\">\n",
       "<h1>Title</h1>\n",
       "<p>My first paragraph</p>\n",
       "</div>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find_all('div')[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b16eddcb-1106-4378-8ed4-feb83bd22ad3",
   "metadata": {},
   "source": [
    "How do we access the p tag in the first element?\n",
    "\n",
    "We do it like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "62d6d984-70f4-406f-88c4-07b9be65f8e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<p>My first paragraph</p>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_item = soup.find_all('div')[0]\n",
    "first_item.find('p')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19907c36-ce1b-4d7a-af47-eaf90fc649c6",
   "metadata": {},
   "source": [
    "By adding .text we can return the actual text string, and not the tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "165de992-abbb-4500-8be4-aef18a2781d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'My first paragraph'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_item = soup.find_all('div')[0]\n",
    "first_item.find('p').text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3532a6d1-1c6b-41f6-8183-fb577b4c1aa6",
   "metadata": {},
   "source": [
    "When you have a list, then you can also loop through the list using a for loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "886ba17a-3326-4060-b0a3-52862043ebf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Title\n",
      "My first paragraph\n",
      "\n",
      "\n",
      "My first headline\n",
      "My second paragraph\n",
      "\n",
      "\n",
      "My second headline\n",
      "My third paragraph\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in soup.find_all('div'):\n",
    "    print (i.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e437cb-3a18-4d55-81bb-5c7abc1b5080",
   "metadata": {},
   "source": [
    "If you are only interested in the div tags that have a particular attribute, you can add an argument to .find or .find_all. The argument takes the form of a dictionary. The key is the attribute and the value is the name of the attribute you want to extract."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0d31beec-0bec-4afb-9432-30be75b5e946",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<div class=\"other\">\n",
       " <h2>My first headline</h2>\n",
       " <p>My second paragraph</p>\n",
       " </div>]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find_all('div', {'class': 'other'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "777cbbb4-6913-450e-bf51-ef1a4a362108",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<div class=\"content\">\n",
       " <h1>Title</h1>\n",
       " <p>My first paragraph</p>\n",
       " </div>,\n",
       " <div class=\"content\">\n",
       " <h2>My second headline</h2>\n",
       " <p>My third paragraph</p>\n",
       " </div>]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_content = soup.find_all('div', {'class': 'content'})\n",
    "all_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5a3613de-bd05-4768-a714-3be451ff0b5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My first paragraph\n",
      "My third paragraph\n"
     ]
    }
   ],
   "source": [
    "for i in all_content:\n",
    "    p_tag = i.find('p')\n",
    "    print (p_tag.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "78ec95fb-dcf3-4051-92e7-0a316c7ae344",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title\n",
      "My first paragraph\n",
      "My first headline\n",
      "My second paragraph\n",
      "My second headline\n",
      "My third paragraph\n"
     ]
    }
   ],
   "source": [
    "all_text_tags = soup.find_all(['h1', 'h2', 'p'])\n",
    "for i in all_text_tags:\n",
    "    print (i.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e3be7e6-16fe-41fd-800b-7313531dff11",
   "metadata": {},
   "source": [
    "## Scrape a webpage\n",
    "\n",
    "When scraping data from the web, you should behave properly. Here are some rules of thumb that you can take with you.\n",
    "\n",
    "\n",
    "1. Take only the data you need, so think about whether you can save the data that you harvest instead of harvesting the same data many times\n",
    "2. Be careful not to scrape material that you are not allowed to use\n",
    "3. Do not try to get, what you can not access\n",
    "4. Slow down! Avoid sending too many requests in a short period. Use a timer to add a pause between the requests.\n",
    "5. Add your name and email to your header so web managers know that the request is not from a evil minded person.\n",
    "6. Go to [https://httpbin.org/headers](https://httpbin.org/headers) to get information about user-agent to add to your script.\n",
    "\n",
    "\n",
    "To scrape webpages using BeautifulSoup you got to add another library. That is [the Requests library](https://requests.readthedocs.io/en/latest/).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5021f192-b341-4225-b1dc-13f8453d1454",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as bs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd492e09-f914-403c-8f40-d970533f1a7b",
   "metadata": {},
   "source": [
    "Read the infomation that you send to the webpage when making a request by inspecting the response from https://httpbin.org/headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "26ab0fa4-46ea-41e0-9d28-6075f6f2f0d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\\n  \"headers\": {\\n    \"Accept\": \"*/*\", \\n    \"Accept-Encoding\": \"gzip, deflate\", \\n    \"Host\": \"httpbin.org\", \\n    \"User-Agent\": \"python-requests/2.32.3\", \\n    \"X-Amzn-Trace-Id\": \"Root=1-675991c7-517351fc357bb29a58aed28c\"\\n  }\\n}\\n'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = 'https://httpbin.org/headers'\n",
    "\n",
    "response = requests.get(url)\n",
    "\n",
    "response.text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb28b10c-ec59-4b00-b6c2-5f7651de950b",
   "metadata": {},
   "source": [
    "## Customize your header\n",
    "\n",
    "When running the code below you can see the User-Agent tells that you are using python requests.\n",
    "\n",
    "Some guides to webscraping will encouraged you to change the value of the User-Agent, because some websites will block requests from headers containing python requests.\n",
    "\n",
    "If you wish to change the value of User-Agent then open [https://httpbin.org/headers](https://httpbin.org/headers) in your default browser to get the data to add to your customized header. \n",
    "\n",
    "I would encourage you to add your name and email to your header. In this way web managers would know that the request is not from a evil minded person. They also have a chance to reach out to you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f317242f-46ee-4ff9-89ba-1bd02cbab7df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\\n  \"headers\": {\\n    \"Accept\": \"*/*\", \\n    \"Accept-Encoding\": \"gzip, deflate\", \\n    \"Host\": \"httpbin.org\", \\n    \"User-Agent\": \"python-requests/2.32.3\", \\n    \"X-Amzn-Trace-Id\": \"Root=1-675991c8-16355f2c0d5a0e03737defc9\"\\n  }\\n}\\n'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = 'https://httpbin.org/headers'\n",
    "\n",
    "response = requests.get(url)\n",
    "\n",
    "response.text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6cd768b-1676-49d5-b30e-b830420ad287",
   "metadata": {},
   "source": [
    "To customize your header you can do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "15528908-e29a-4a44-a96f-c36f27207fba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\\n  \"headers\": {\\n    \"Accept\": \"*/*\", \\n    \"Accept-Encoding\": \"gzip, deflate\", \\n    \"E-Mail\": \"add e-mail\", \\n    \"Host\": \"httpbin.org\", \\n    \"Name\": \"Add your name here. Collects data in connection with a course\", \\n    \"User-Agent\": \"python-requests/2.32.3\", \\n    \"X-Amzn-Trace-Id\": \"Root=1-675991c8-6f65bddb75dfa3060c098965\"\\n  }\\n}\\n'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = 'https://httpbin.org/headers'\n",
    "\n",
    "headers={'name': 'Add your name here. Collects data in connection with a course', 'e-mail': 'add e-mail'}\n",
    "\n",
    "response = requests.get(url, headers=headers)\n",
    "\n",
    "response.text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "337c1874-be25-4ee2-8cfb-2ab7fcfeec9b",
   "metadata": {},
   "source": [
    "## Scrape a wiki page\n",
    "\n",
    "In the script below we:\n",
    "\n",
    "- Store the url to the wikipedia page as a string in a variable called url.\n",
    "\n",
    "- Customize the header. It got to be build like a dictionary.\n",
    "\n",
    "- Use the requests.get method and add the url and the header as arguments.\n",
    "\n",
    "- Download the HTML by storing it into the variable called soup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bc3c3c3b-bb2b-4cf8-a7d4-a226b9268b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://en.wikipedia.org/wiki/2019%E2%80%932020_Hong_Kong_protests'\n",
    "\n",
    "headers={'name': 'Add your name here. Collects data in connection with a course', 'e-mail': 'add e-mail'}\n",
    "\n",
    "response = requests.get(url, headers=headers)\n",
    "\n",
    "soup = bs(response.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f4ef35-153a-434b-9595-05110be208b7",
   "metadata": {},
   "source": [
    "## Parse / extract data from the HTML\n",
    "\n",
    "When we parse data from the HTML we have to use the inspect tool. I use Firefox and when I right click on the wikipedia page I can choose a tool called Inspect from the popup window. Take a look at this video for more information on how to use Inspect: [Python for Digital Humanities - 21: Beautiful Soup](https://www.youtube.com/watch?v=_tdW6n7lUX4&t=50s)\n",
    "\n",
    "The title tag is in the 'h1' (headline) tag that has a attribute called 'id' with the value 'firstHeading'.\n",
    "\n",
    "We can use .find and add the dictionary as an argument. Then we store the returned values in a variable (title_tag). In the second line we can add .text to the variable to extract the text string for the tag. We send the textstring to the variable title and print it.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3e38c95d-3146-4842-96cf-9f401104cdd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2019â€“2020 Hong Kong protests'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title_tag = soup.find('h1', {'id': \"firstHeading\"})\n",
    "title = title_tag.text\n",
    "\n",
    "title"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a4e3f0e-181d-4913-b2ba-b28fba7a9d9f",
   "metadata": {},
   "source": [
    "The rest of the content is embedded in h2 and p tags.\n",
    "\n",
    "Let us extract the text from the h1, h2 og p tags and store the text data in a txt fil."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "51ed5294-a6a7-417b-a244-c3e8bedce753",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_text_tags = soup.find_all(['h1', 'h2', 'p'])\n",
    "all_text = [i.text for i in all_text_tags]\n",
    "all_text = ' '.join(all_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c2e78f3b-2267-4358-91e3-eb2afe7787dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store text as txt\n",
    "with open(title+'.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write(all_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea6a21e6-f6df-494b-b8a4-b58191a5c762",
   "metadata": {},
   "source": [
    "Now you are done!\n",
    "\n",
    "You can clean the text before you start analysing it. To do so take a look at the \"Text preprocessing pipeline\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a8c2703-13e0-4a39-87db-14543f02fbdf",
   "metadata": {},
   "source": [
    "## Read more:\n",
    "\n",
    "Dr. W.J.B. Mattingly [Introduction to Python for Humanists](https://python-textbook.pythonhumanities.com/intro.html)\n",
    "\n",
    "Dr. W.J.B. Mattingly [Python Tutorials for Digital Humanities](https://www.youtube.com/@python-programming)\n",
    "\n",
    "John Watson Rooney [Request Headers for Web Scraping](https://www.google.com/search?q=add+Scraping-Info+to+header&client=firefox-b-d&sca_esv=da052a448206c26a&sxsrf=ADLYWIJWhu8dXpWqbQPdYUsEBcHUwtVEig%3A1733822113466&ei=oQZYZ-iVHM3LwPAPkeDawAk&ved=0ahUKEwio6rTZ7pyKAxXNJRAIHRGwFpgQ4dUDCBA&uact=5&oq=add+Scraping-Info+to+header&gs_lp=Egxnd3Mtd2l6LXNlcnAiG2FkZCBTY3JhcGluZy1JbmZvIHRvIGhlYWRlcjIFECEYoAFI3GVQrAZY7GRwAXgBkAEBmAGRAaAB5gmqAQQxMS4zuAEDyAEA-AEBmAIOoAKECcICChAAGLADGNYEGEfCAggQABiABBiiBMICBxAjGLACGCfCAgYQABgNGB7CAgcQIRigARgKmAMAiAYBkAYIkgcEMTIuMqAH7yM&sclient=gws-wiz-serp#fpstate=ive&vld=cid:fb61f8c6,vid:Oz902cJcCUg,st:163\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aff9379-42b5-46ca-a55e-fe5145c9835f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}